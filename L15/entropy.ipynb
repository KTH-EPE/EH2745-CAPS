{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy - information content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This small notebook introduces and provides some examples of entropy calculation. The examples include calcualting entropy from scratch, without libraries, and also use of the entropy function from the Scipy package.\n",
    "\n",
    "First, we import some basic packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy of a dataset in Information Theory (not to be confused with entropy in physics) is a measure of randomness in the dataset. It was introduced by Claude Shannon in 1948, and the entropy of a sataset is defined as per below.\n",
    "\n",
    "![title](entropy.svg)\n",
    "\n",
    "Or in plain english, the entropy for a dataset is the negative sum of the product of the probability of the value and the logaritm (of arbitrary base) of the probability of value. \n",
    "\n",
    "Since the entropy is our (or rather Shannon's) definition, we can choose the base of the logaritm arbitrarily. However, by convetion the base 2 is often chosen, and then the \"unit of measurement\" for entropy is \"bits\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try something out with our new-found knowledge, we first need a dataset to work with. Lets set up some Python lists representing various data series.\n",
    "\n",
    "Tossing a coin, gives two possible outcomes, an if the coin is fair, there is a 50-50 probability for each result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_toss = [1/2,1/2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rolling a 6 sided die, similarly provides 6 possible outcomes with 1/6 chance of each outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "die_roll = [1/6,1/6,1/6,1/6,1/6,1/6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the coin we are tossing is unfair, providing a 70% chance of e.g. heads and 30% of tails, the data series becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_coin_toss = [7/10, 3/10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for a loaded die, where the chances of rolling a 6 is doubled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_die_roll = [2/15,2/15,2/15,2/15,2/15,1/3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for a completely random series of 100 numbers, we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "random_data = []\n",
    "# Create a series of 100 random values between 1 and 99)\n",
    "for n in range(0,99):\n",
    "    random_data.append(random.randint(1,10))\n",
    "    sum+=random_data[n]\n",
    "# Normalise the values\n",
    "for m in range (0,99):\n",
    "    random_data[m]/=sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we have some dataseries to work with, lets define the entropy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(list):\n",
    "    #The input to the function is a list of values, and the output is the entropy of that\n",
    "    # list using base 2 for the logarithm\n",
    "    entr=0\n",
    "    for n in range (len(list)):\n",
    "        entr+=list[n]*math.log(list[n],2)\n",
    "    return -entr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the entropy of the coin toss - how random is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(entropy(coin_toss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And tossing a loaded coin, how random is that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8812908992306927\n"
     ]
    }
   ],
   "source": [
    "print(entropy(loaded_coin_toss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears, that the randomness of tossing a loaded coin is less than for a fair coin - this appears intuitvely right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the die vs the loaded die?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.584962500721156\n",
      "2.4662478973127313\n"
     ]
    }
   ],
   "source": [
    "print (entropy(die_roll))\n",
    "print (entropy(loaded_die_roll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret the above numbers as ther being more randomness in the die roll versus the coin toss - that gain makes sense intuitively, since there are more possible outcomes in the die rolls as compared to the coin toss. Similarly, it appears reasonable that tha randomness of the loaded die should be less than for the fair die.\n",
    "\n",
    "Just for testing , what would the entropy of a very unfair die be - one that almost always rolls a \"6\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totally_unfair_die = [0.00001,0.00001,0.00001,0.00001,0.00001,0.99995]\n",
    "print (entropy(totally_unfair_die))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the entropy tends towards a very small number for a die with almoste certain outcome.\n",
    "\n",
    "Finally, what about the completely random datset we generated, what can we expect the entropy be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (entropy(random_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy of the dataseries will vary depending on how the probabilities for the different values were generatd in our random number series. But in total, the entropy is higher than for the dat series with fewer possible outcomes that we studied at start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets create a very long and random dataseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "very_long_random_data = []\n",
    "# Create a series of 10000 random values between 1 and 99)\n",
    "for n in range(0,9999):\n",
    "    very_long_random_data.append(random.randint(1,10))\n",
    "    sum+=very_long_random_data[n]\n",
    "# Normalise the values\n",
    "for m in range (0,9999):\n",
    "    very_long_random_data[m]/=sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (entropy(random_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, python provides an \"out of the box\" entropy calculation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "print (entropy(die_roll,base=2))\n",
    "print (entropy(loaded_die_roll,base=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, the entropy of a datseries is a measure of the randomness in that dataseries. Comparing entropy across  dataseries of different size (like a die-Roll with a coin_toss) provides some qualitative measure of how random the dataseries are in relation to eachother, but that is about it. For dataseries of same size, the entropy measure proivides a bit more insights since we can say which of two same size data series that is more or less random with certainy by looking at the entropy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
